{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3166e69e",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "\n",
    "The Taxi and Limousine Commission (TLC) of New York City collects trip record data from licensed taxis and for-hire vehicles (FHVs) and provides it to the public. The data includes details such as pick-up and drop-off times, locations, passenger counts, and payment information for each trip. As a data engineer, your task is to build a batch data processing pipeline using PySpark to process and analyze this data to gain insights into taxi and FHV trips in New York City."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c335dd09",
   "metadata": {},
   "source": [
    "### Goals:\n",
    "\n",
    "Data ingestion: Download the trip record data from the NYC TLC website and ingest it into the pipeline for further processing.\n",
    "\n",
    "Data cleaning and validation: Perform data quality checks and validation to ensure that the data is clean and consistent. Identify and remove duplicates, null values, and other data quality issues that may impact downstream analysis.\n",
    "\n",
    "Data transformation: Transform the raw trip record data into a format that is optimized for analysis. This may include aggregating the data by time periods, geographical regions, and other factors of interest.\n",
    "\n",
    "Data analysis: Use PySpark to perform statistical analysis, data exploration, and data visualization to gain insights into taxi and FHV trips in New York City. This may include identifying popular pick-up and drop-off locations, peak trip times, and other patterns and trends in the data.\n",
    "\n",
    "Data storage: Store the processed and analyzed data in a suitable data storage system such as Hadoop Distributed File System (HDFS) or Apache Cassandra for future use.\n",
    "\n",
    "Automation and scheduling: Automate the data processing pipeline using tools such as Apache Airflow or Apache Oozie. Schedule the pipeline to run at regular intervals to ensure that the data is up to date and accurate.\n",
    "\n",
    "---\n",
    "\n",
    "The overall goal of the project is to build a batch data processing pipeline using PySpark to extract insights from the NYC TLC trip record data. The pipeline should be scalable, efficient, and automated to enable easy data processing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1cf91",
   "metadata": {},
   "source": [
    "### Import Libraries and Intiate Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2475ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5775a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/15 21:17:49 WARN Utils: Your hostname, joker021-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/05/15 21:17:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/15 21:17:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"nyc_batch_pipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fba643c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>nyc_batch_pipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdb42e14fa0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212656a",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e32b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Parsing from config file\n",
    "conf = configparser.ConfigParser()\n",
    "conf.read(\"config\")\n",
    "data_source_path = conf.get(\"DATASOURCE PATH\", \"PATH\")\n",
    "\n",
    "# Reading the DataSource from PySpark\n",
    "df_full = spark.read.parquet(data_source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abda0891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Partitons: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"No of Partitons: {df_full.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1306b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.1\n",
    "df = df_full.sample(fraction=frac, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae8bd3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_name: string (nullable = true)\n",
      " |-- Trip_Pickup_DateTime: string (nullable = true)\n",
      " |-- Trip_Dropoff_DateTime: string (nullable = true)\n",
      " |-- Passenger_Count: long (nullable = true)\n",
      " |-- Trip_Distance: double (nullable = true)\n",
      " |-- Start_Lon: double (nullable = true)\n",
      " |-- Start_Lat: double (nullable = true)\n",
      " |-- Rate_Code: double (nullable = true)\n",
      " |-- store_and_forward: double (nullable = true)\n",
      " |-- End_Lon: double (nullable = true)\n",
      " |-- End_Lat: double (nullable = true)\n",
      " |-- Payment_Type: string (nullable = true)\n",
      " |-- Fare_Amt: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- Tip_Amt: double (nullable = true)\n",
      " |-- Tolls_Amt: double (nullable = true)\n",
      " |-- Total_Amt: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21617cf",
   "metadata": {},
   "source": [
    "### Data Cleaning And Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c037c",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "964441b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Rows: 1409962\n",
      "No of cols: 18\n"
     ]
    }
   ],
   "source": [
    "no_of_row = df.count()\n",
    "print(f\"No of Rows: {no_of_row}\")\n",
    "print(f\"No of cols: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52345774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "null_count = df.\\\n",
    "select(\n",
    "    [F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    ").collect()[0]\\\n",
    ".asDict()\n",
    "null_col_list = [c for c in null_count if null_count[c] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84284dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vendor_name': 0,\n",
       " 'Trip_Pickup_DateTime': 0,\n",
       " 'Trip_Dropoff_DateTime': 0,\n",
       " 'Passenger_Count': 0,\n",
       " 'Trip_Distance': 0,\n",
       " 'Start_Lon': 0,\n",
       " 'Start_Lat': 0,\n",
       " 'Rate_Code': 1409962,\n",
       " 'store_and_forward': 1409835,\n",
       " 'End_Lon': 0,\n",
       " 'End_Lat': 0,\n",
       " 'Payment_Type': 0,\n",
       " 'Fare_Amt': 0,\n",
       " 'surcharge': 0,\n",
       " 'mta_tax': 1409962,\n",
       " 'Tip_Amt': 0,\n",
       " 'Tolls_Amt': 0,\n",
       " 'Total_Amt': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d42230a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-------+\n",
      "|Rate_Code|store_and_forward|mta_tax|\n",
      "+---------+-----------------+-------+\n",
      "|  1409962|          1409835|1409962|\n",
      "+---------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We could see below three cols have huge no of Null Values\n",
    "# The Rate_Code and mta_tax is completely null\n",
    "# store_and_forward have few rows present\n",
    "null_col_list = [\"Rate_Code\", \"store_and_forward\", \"mta_tax\"]\n",
    "df.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in null_col_list]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "244ddec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-------+\n",
      "|Rate_Code|store_and_forward|mta_tax|\n",
      "+---------+-----------------+-------+\n",
      "|        0|                2|      0|\n",
      "+---------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking no of Distinct values in Null having columns\n",
    "df.select([F.countDistinct(F.col(c)).alias(c) for c in null_col_list]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa89a984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+\n",
      "|store_and_forward|  count|\n",
      "+-----------------+-------+\n",
      "|              0.0|    126|\n",
      "|             null|1409835|\n",
      "|              1.0|      1|\n",
      "+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count of distinct values on store_and_forward, we could see the values are very small compared to total rows\n",
    "df.select(F.col(\"store_and_forward\")).groupBy('store_and_forward').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e7a6d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the amount of null values is very large compared to total no of records we are dropping those columns\n",
    "df_not_null = df.drop(*null_col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36566bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[vendor_name: string, Trip_Pickup_DateTime: string, Trip_Dropoff_DateTime: string, Passenger_Count: bigint, Trip_Distance: double, Start_Lon: double, Start_Lat: double, Rate_Code: double, store_and_forward: double, End_Lon: double, End_Lat: double, Payment_Type: string, Fare_Amt: double, surcharge: double, mta_tax: double, Tip_Amt: double, Tolls_Amt: double, Total_Amt: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d6705d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_name: string (nullable = true)\n",
      " |-- Trip_Pickup_DateTime: string (nullable = true)\n",
      " |-- Trip_Dropoff_DateTime: string (nullable = true)\n",
      " |-- Passenger_Count: long (nullable = true)\n",
      " |-- Trip_Distance: double (nullable = true)\n",
      " |-- Start_Lon: double (nullable = true)\n",
      " |-- Start_Lat: double (nullable = true)\n",
      " |-- End_Lon: double (nullable = true)\n",
      " |-- End_Lat: double (nullable = true)\n",
      " |-- Payment_Type: string (nullable = true)\n",
      " |-- Fare_Amt: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- Tip_Amt: double (nullable = true)\n",
      " |-- Tolls_Amt: double (nullable = true)\n",
      " |-- Total_Amt: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_not_null.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134eca3",
   "metadata": {},
   "source": [
    "#### DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d44ad9f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|Trip_Pickup_DateTime|Trip_Dropoff_DateTime|\n",
      "+--------------------+---------------------+\n",
      "| 2009-01-05 10:23:13|  2009-01-05 10:33:56|\n",
      "| 2009-01-03 17:08:30|  2009-01-03 17:16:31|\n",
      "| 2009-01-19 21:26:03|  2009-01-19 21:51:42|\n",
      "+--------------------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Currently the DateTime col are in String format, will check the format\n",
    "date_col = [\"Trip_Pickup_DateTime\", \"Trip_Dropoff_DateTime\"]\n",
    "df_not_null.select(date_col).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42f40836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could see the format is %Y-%M-%D %H:%M:%s\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "date_dict_map = {date_c: F.to_timestamp(F.col(date_c)) for date_c in date_col}\n",
    "df_date_parsed = df_not_null.withColumns(date_dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46fbce92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_name: string (nullable = true)\n",
      " |-- Trip_Pickup_DateTime: timestamp (nullable = true)\n",
      " |-- Trip_Dropoff_DateTime: timestamp (nullable = true)\n",
      " |-- Passenger_Count: long (nullable = true)\n",
      " |-- Trip_Distance: double (nullable = true)\n",
      " |-- Start_Lon: double (nullable = true)\n",
      " |-- Start_Lat: double (nullable = true)\n",
      " |-- End_Lon: double (nullable = true)\n",
      " |-- End_Lat: double (nullable = true)\n",
      " |-- Payment_Type: string (nullable = true)\n",
      " |-- Fare_Amt: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- Tip_Amt: double (nullable = true)\n",
      " |-- Tolls_Amt: double (nullable = true)\n",
      " |-- Total_Amt: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date_parsed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8713bef",
   "metadata": {},
   "source": [
    "#### Distinct Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6e00422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joker021/.local/lib/python3.10/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "/home/joker021/.local/lib/python3.10/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_list` loads all data into the driver's memory. It should only be used if the resulting list is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "string_cols = [f.name for f in df_date_parsed.schema.fields if isinstance(f.dataType, F.StringType)]\n",
    "distinct_count = df_date_parsed.select([F.countDistinct(F.col(c)).alias(c) for c in string_cols])\n",
    "distinct_count_pd = distinct_count.pandas_api().transpose()\n",
    "dist_cols = distinct_count_pd[distinct_count_pd[0]<50].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08211500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|vendor_name|Payment_Type|\n",
      "+-----------+------------+\n",
      "|          3|           6|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date_parsed.select([F.countDistinct(F.col(c)).alias(c) for c in dist_cols]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fb5983c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|vendor_name|\n",
      "+-----------+\n",
      "|        CMT|\n",
      "|        VTS|\n",
      "|        DDS|\n",
      "+-----------+\n",
      "\n",
      "+------------+\n",
      "|Payment_Type|\n",
      "+------------+\n",
      "|   No Charge|\n",
      "|        CASH|\n",
      "|      Credit|\n",
      "|        Cash|\n",
      "|     Dispute|\n",
      "|      CREDIT|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View Distinct Values\n",
    "for c in dist_cols:\n",
    "    df_date_parsed.select(c).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f498fff",
   "metadata": {},
   "source": [
    "#### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecc7846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried distinct method and directly dropDup but both consumed lot of memory, \n",
    "# hence applying a tranformation and removing the duplicates which resulted in same result as direct dropDuplicates\n",
    "# but less mem consumption\n",
    "\n",
    "# We are concating all cols, and then checking dup based on concated col\n",
    "df_drop_by_concat = df_date_parsed\\\n",
    ".withColumn(\"concat_cols\", F.concat_ws(\"||\", *df_date_parsed.columns))\\\n",
    ".dropDuplicates([\"concat_cols\"])\\\n",
    ".drop(\"concat_cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00e55f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking count after duplicates\n",
    "cnt_after_drop = df_drop_by_concat.count()\n",
    "\n",
    "# No of Duplciates dropped\n",
    "no_of_row - cnt_after_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3e41b",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d42b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Duration Columns\n",
    "df_cleaned = df_drop_by_concat.withColumn(\n",
    "    \"duration\", \n",
    "    F.col(\"Trip_Dropoff_DateTime\").cast(\"long\") - F.col(\"Trip_Pickup_DateTime\").cast(\"long\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab8efe",
   "metadata": {},
   "source": [
    "### Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b1a0c",
   "metadata": {},
   "source": [
    "#### Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32b04c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Schema for Validation\n",
    "validate_schema = StructType(\n",
    "    [\n",
    "        StructField('vendor_name', StringType(), True), \n",
    "        StructField('Trip_Pickup_DateTime', TimestampType(), True), \n",
    "        StructField('Trip_Dropoff_DateTime', TimestampType(), True), \n",
    "        StructField('Passenger_Count', LongType(), True), \n",
    "        StructField('Trip_Distance', DoubleType(), True), \n",
    "        StructField('Start_Lon', DoubleType(), True), \n",
    "        StructField('Start_Lat', DoubleType(), True), \n",
    "        StructField('End_Lon', DoubleType(), True), \n",
    "        StructField('End_Lat', DoubleType(), True), \n",
    "        StructField('Payment_Type', StringType(), True), \n",
    "        StructField('Fare_Amt', DoubleType(), True), \n",
    "        StructField('surcharge', DoubleType(), True), \n",
    "        StructField('Tip_Amt', DoubleType(), True), \n",
    "        StructField('Tolls_Amt', DoubleType(), True),\n",
    "        StructField('Total_Amt', DoubleType(), True),\n",
    "        StructField('duration', LongType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8945c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Schema\n",
    "assert validate_schema == df_cleaned.schema, \"schema is not valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb33c67",
   "metadata": {},
   "source": [
    "#### Null Value Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4393f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method cosumes more memory hence commenting the below, It better to do one by one\n",
    "# is_null_values = df_cleaned.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_cleaned.columns]).collect()[0].asDict()\n",
    "# [col for col in is_null_values if is_null_values[col] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfd8667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There No Null columns\n"
     ]
    }
   ],
   "source": [
    "for c in df_cleaned.columns:\n",
    "#     cnt = df_cleaned.select(c).where(F.col(c).isNull()).count()\n",
    "#     Experimenting below\n",
    "    cnt = df_cleaned.where(F.col(c).isNull()).select(c).count()\n",
    "    if cnt > 0:\n",
    "        print(c, cnt)\n",
    "else:\n",
    "    print(\"There No Null columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dc0925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data Range Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50de24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_numeric_cols = [\n",
    "#     f.name \n",
    "#     for f in df_cleaned.schema.fields \n",
    "#     if isinstance(f.dataType, DoubleType) or isinstance(f.dataType, LongType)\n",
    "# ]\n",
    "# bounds = {\n",
    "#     c: dict(\n",
    "#         zip([\"q1\", \"q3\"], df_cleaned.approxQuantile(c, [0.25, 0.75], 0.1))\n",
    "#     )\n",
    "#     for c in df_numeric_cols\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e2806c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in bounds:\n",
    "#     iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "#     bounds[c]['lower'] = bounds[c]['q1'] - (iqr * 1.5)\n",
    "#     bounds[c]['upper'] = bounds[c]['q3'] + (iqr * 1.5)\n",
    "# print(bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f252e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_outlier = df_cleaned.select(\n",
    "#     \"*\",\n",
    "#     *[\n",
    "#         F.when(\n",
    "#             F.col(c).between(bounds[c]['lower'], bounds[c]['upper']),\n",
    "#             0\n",
    "#         ).otherwise(1).alias(c+\"_out\") \n",
    "#         for c in df_numeric_cols\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "feb88a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_outlier.select([F.sum(c).alias(c) for c in df_outlier.columns if c.endswith(\"out\")]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa714d",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15323341",
   "metadata": {},
   "source": [
    "#### Data Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1955f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_name: string (nullable = true)\n",
      " |-- Trip_Pickup_DateTime: timestamp (nullable = true)\n",
      " |-- Trip_Dropoff_DateTime: timestamp (nullable = true)\n",
      " |-- Passenger_Count: long (nullable = true)\n",
      " |-- Trip_Distance: double (nullable = true)\n",
      " |-- Start_Lon: double (nullable = true)\n",
      " |-- Start_Lat: double (nullable = true)\n",
      " |-- End_Lon: double (nullable = true)\n",
      " |-- End_Lat: double (nullable = true)\n",
      " |-- Payment_Type: string (nullable = true)\n",
      " |-- Fare_Amt: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- Tip_Amt: double (nullable = true)\n",
      " |-- Tolls_Amt: double (nullable = true)\n",
      " |-- Total_Amt: double (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c02f87cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_vendor = df_cleaned.select(\"vendor_name\",*numeric_col)\\\n",
    ".groupBy(F.col(\"vendor_name\"))\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2caec9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/15 21:23:38 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "[Stage 144:===========>                                             (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------------------+-----------------+-------------------+------------------+-------------------+------------------+-----------------+\n",
      "|vendor_name|Passenger_Count_avg| Trip_Distance_avg|     Fare_Amt_avg|      surcharge_avg|       Tip_Amt_avg|      Tolls_Amt_avg|     Total_Amt_avg|     duration_avg|\n",
      "+-----------+-------------------+------------------+-----------------+-------------------+------------------+-------------------+------------------+-----------------+\n",
      "|        CMT| 1.3154908167742745|2.5192744812887864|9.542147809189458|                0.0|0.4255705509935462|0.10707211809690544|10.078321851689623|644.7473672663449|\n",
      "|        VTS|  2.104780176092554|2.5648959230269863| 9.41795026516011|0.32534740712102533|0.5058805138342817|0.11915780489762136| 10.37054279399214|729.3283243487712|\n",
      "|        DDS| 1.3510595206406888| 2.710563958938519|9.704646845741683| 0.3411413427152893|0.4095606837409003|0.13657516173458314|10.593646695290929|772.2622704179059|\n",
      "+-----------+-------------------+------------------+-----------------+-------------------+------------------+-------------------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 144:=============================================>           (4 + 1) / 5]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_vendor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41abbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_hour = df_cleaned.select(\"Trip_Pickup_DateTime\", *numeric_col)\\\n",
    ".groupBy(F.hour(F.col(\"Trip_Pickup_DateTime\")).alias(\"Hour\"))\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98a28ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/15 21:23:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:23:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:24:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:24:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:24:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:24:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------+------------------+--------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|Hour|Passenger_Count_avg| Trip_Distance_avg|      Fare_Amt_avg|       surcharge_avg|        Tip_Amt_avg|      Tolls_Amt_avg|     Total_Amt_avg|     duration_avg|\n",
      "+----+-------------------+------------------+------------------+--------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|  12| 1.6641747789254664|2.3139684327859102|  8.95982284312999|2.972430705209184...|0.39772683361819133| 0.1345919595749435| 9.496324589433005|681.5317232667013|\n",
      "|  22| 1.8037264809954692| 2.753081841565801| 9.850208390457944| 0.27377361693237756| 0.5254443965686075|0.08620836434381855|10.738692075678665| 678.570227323175|\n",
      "|   1|  1.802593842795868|  3.03510776376234|10.250417755894517|  0.2565036420395421| 0.5168655617877718|0.05219106114058011|  11.0793611837263| 689.831527118601|\n",
      "+----+-------------------+------------------+------------------+--------------------+-------------------+-------------------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 150:==========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_hour.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f59399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_date = df_cleaned.select(\n",
    "    F.to_date(\"Trip_Pickup_DateTime\", \"yyyy-MM-dd\").alias(\"date\"), \n",
    "    *numeric_col\n",
    ")\\\n",
    ".groupBy(\"date\")\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dcdaa437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/15 21:24:15 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "23/05/15 21:24:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:24:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:24:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:24:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:24:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|      date|Passenger_Count_avg| Trip_Distance_avg|     Fare_Amt_avg|      surcharge_avg|        Tip_Amt_avg|      Tolls_Amt_avg|     Total_Amt_avg|     duration_avg|\n",
      "+----------+-------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|2009-01-01| 1.8418353329059438| 2.880220105626273|9.835373813230772|0.06561956223097354|0.33303568702872666|0.12587080623988736|10.368288915346342|629.5846689257259|\n",
      "|2009-01-30| 1.6884121126708327|2.5138183099375095|9.603914551152494|0.22340386077475105| 0.5020155024384818|0.11263689802881714|10.445140838540988|738.3874497005211|\n",
      "|2009-01-22| 1.6341111956130168|2.5263746747788525|9.592409838690319| 0.2132149861906096| 0.5204180842973222|0.12623283833006493|10.455622823519986|715.0952047392227|\n",
      "+----------+-------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_date.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ea4c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_week = df_cleaned.select(\"Trip_Pickup_DateTime\", *numeric_col)\\\n",
    ".groupBy(F.weekofyear(F.col(\"Trip_Pickup_DateTime\")).alias(\"WeekOfYear\"))\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "854c2f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/15 21:24:36 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "23/05/15 21:24:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:24:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:24:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:24:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:24:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|WeekOfYear|Passenger_Count_avg| Trip_Distance_avg|     Fare_Amt_avg|      surcharge_avg|        Tip_Amt_avg|      Tolls_Amt_avg|     Total_Amt_avg|     duration_avg|\n",
      "+----------+-------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|         1|  1.848428970842764|2.8394042670749577| 9.88707196112367| 0.1321794701105046|0.37229097323924903|0.13152762614831554|10.527513380375462|675.2425575822127|\n",
      "|         3| 1.6847972481071636|2.4436628415599015|9.399485385119249|0.18399188264414676|0.47540322874199165| 0.1129132692680994|10.173902154921254|689.8315587992623|\n",
      "|         5| 1.6651743047129717|2.4685146411891914|9.362059425604409|0.19905577886381662|0.49271878749262815|0.10638172833815433|10.163310603338951|704.8975726806256|\n",
      "+----------+-------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_week.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4727a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_dayofmonth = df_cleaned.select(\"Trip_Pickup_DateTime\", *numeric_col)\\\n",
    ".groupBy(F.dayofmonth(F.col(\"Trip_Pickup_DateTime\")).alias(\"DayOfMonth\"))\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f630d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/15 21:24:55 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "23/05/15 21:24:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:25:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:25:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:25:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:25:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 168:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|DayOfMonth|Passenger_Count_avg| Trip_Distance_avg|     Fare_Amt_avg|      surcharge_avg|        Tip_Amt_avg|      Tolls_Amt_avg|     Total_Amt_avg|     duration_avg|\n",
      "+----------+-------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|        31| 1.7918311626019259|2.4959505405305684| 9.24101280301073|0.12173006678227502|0.43194019112275395|0.06843393720252351| 9.864978969117798| 680.225657676272|\n",
      "|        28| 1.6335729847494553| 2.350026187363829| 9.13503071895426|0.21962962962962962|  0.518329193899782|0.09630675381263602|  9.97617712418301|719.3332897603486|\n",
      "|        26| 1.6117397127418833|2.5066151803445242|9.272378423313162|0.21411582690620237|  0.479838590495543|0.12951623331639991|10.097802844871353|654.2429455502702|\n",
      "+----------+-------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_dayofmonth.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4548d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_dayofweek = df_cleaned.select(\"Trip_Pickup_DateTime\", *numeric_col)\\\n",
    ".groupBy(F.dayofweek(F.col(\"Trip_Pickup_DateTime\")).alias(\"dayofweek\"))\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b96d7965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/15 21:25:13 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "23/05/15 21:25:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:25:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:25:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:25:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/15 21:25:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 174:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|dayofweek|Passenger_Count_avg|Trip_Distance_avg|     Fare_Amt_avg|      surcharge_avg|        Tip_Amt_avg|      Tolls_Amt_avg|     Total_Amt_avg|     duration_avg|\n",
      "+---------+-------------------+-----------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|        1| 1.7937177911138296|2.845371548993836|9.775786608370222|0.11217321795159053|0.46167743576389947|0.12997583765812917|10.481227555970078|664.9007103365167|\n",
      "|        6| 1.7096297782682854|2.538974576101145|9.577798896357946|0.22210895956656967| 0.4565002909601689|0.11610362195244035|10.375630019062903|716.8463248720778|\n",
      "|        3| 1.6294155571128242|2.499379582340564|9.361223545900499|0.20972950245269797|0.48244339173090384|0.11964384022424589|10.174955851436597| 692.901359495445|\n",
      "+---------+-------------------+-----------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_dayofweek.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97fea850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt_per_payment_type = df_cleaned.select(\"Payment_Type\")\\\n",
    ".groupBy(F.col(\"Payment_Type\"))\\\n",
    ".count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b06489e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 178:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|Payment_Type| count|\n",
      "+------------+------+\n",
      "|   No Charge|  3990|\n",
      "|        CASH|602997|\n",
      "|      Credit|287365|\n",
      "+------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 180:======================>                                  (2 + 3) / 5]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_cnt_per_payment_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "955301a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_payment_type = df_cleaned.select(\"Payment_Type\",*numeric_col)\\\n",
    ".groupBy(F.col(\"Payment_Type\"))\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9eee393e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 186:>                                                        (0 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+------------------+------------------+-------------------+--------------------+-------------------+------------------+-----------------+\n",
      "|Payment_Type|Passenger_Count_avg| Trip_Distance_avg|      Fare_Amt_avg|      surcharge_avg|         Tip_Amt_avg|      Tolls_Amt_avg|     Total_Amt_avg|     duration_avg|\n",
      "+------------+-------------------+------------------+------------------+-------------------+--------------------+-------------------+------------------+-----------------+\n",
      "|   No Charge| 1.2150375939849625| 2.456716791979948| 10.36890977443609|                0.0|0.007982456140350877| 0.1405087719298246|10.571872180451125|658.1862155388471|\n",
      "|        CASH|  2.029670131028844|2.3973074973838964| 8.900942475667307|0.32386894130484895|2.487574565047587...|0.09005142645817063| 9.315773710316913|658.9716797927684|\n",
      "|      Credit| 1.6813947418787953|3.2070011135663887|11.407868390374597| 0.1797818105893202|   2.148675099612018| 0.2152307692307591|13.958098341830146|896.4198980390793|\n",
      "+------------+-------------------+------------------+------------------+-------------------+--------------------+-------------------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 186:===========>                                             (1 + 4) / 5]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_payment_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d553cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt_payment_cnt_per_vendor = df_cleaned\\\n",
    ".select(\"vendor_name\", \"Payment_Type\")\\\n",
    ".groupBy(\"vendor_name\", \"Payment_Type\")\\\n",
    ".count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc45bbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 192:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------+\n",
      "|vendor_name|Payment_Type| count|\n",
      "+-----------+------------+------+\n",
      "|        VTS|        CASH|532456|\n",
      "|        DDS|      CREDIT| 15866|\n",
      "|        CMT|      Credit|134386|\n",
      "|        DDS|        CASH| 70541|\n",
      "|        CMT|   No Charge|  3990|\n",
      "|        CMT|        Cash|498909|\n",
      "|        VTS|      Credit|152979|\n",
      "|        CMT|     Dispute|   835|\n",
      "+-----------+------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 192:==========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_cnt_payment_cnt_per_vendor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119da26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbaca11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
