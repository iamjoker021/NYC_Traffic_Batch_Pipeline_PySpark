{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3166e69e",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "\n",
    "The Taxi and Limousine Commission (TLC) of New York City collects trip record data from licensed taxis and for-hire vehicles (FHVs) and provides it to the public. The data includes details such as pick-up and drop-off times, locations, passenger counts, and payment information for each trip. As a data engineer, your task is to build a batch data processing pipeline using PySpark to process and analyze this data to gain insights into taxi and FHV trips in New York City."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c335dd09",
   "metadata": {},
   "source": [
    "### Goals:\n",
    "\n",
    "Data ingestion: Download the trip record data from the NYC TLC website and ingest it into the pipeline for further processing.\n",
    "\n",
    "Data cleaning and validation: Perform data quality checks and validation to ensure that the data is clean and consistent. Identify and remove duplicates, null values, and other data quality issues that may impact downstream analysis.\n",
    "\n",
    "Data transformation: Transform the raw trip record data into a format that is optimized for analysis. This may include aggregating the data by time periods, geographical regions, and other factors of interest.\n",
    "\n",
    "Data analysis: Use PySpark to perform statistical analysis, data exploration, and data visualization to gain insights into taxi and FHV trips in New York City. This may include identifying popular pick-up and drop-off locations, peak trip times, and other patterns and trends in the data.\n",
    "\n",
    "Data storage: Store the processed and analyzed data in a suitable data storage system such as Hadoop Distributed File System (HDFS) or Apache Cassandra for future use.\n",
    "\n",
    "Automation and scheduling: Automate the data processing pipeline using tools such as Apache Airflow or Apache Oozie. Schedule the pipeline to run at regular intervals to ensure that the data is up to date and accurate.\n",
    "\n",
    "---\n",
    "\n",
    "The overall goal of the project is to build a batch data processing pipeline using PySpark to extract insights from the NYC TLC trip record data. The pipeline should be scalable, efficient, and automated to enable easy data processing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1cf91",
   "metadata": {},
   "source": [
    "### Import Libraries and Intiate Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2841e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install geospark\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2475ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType\n",
    "\n",
    "import geospark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5775a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/16 12:02:00 WARN Utils: Your hostname, joker021-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/05/16 12:02:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/16 12:02:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"nyc_batch_pipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fba643c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>nyc_batch_pipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa66d0e5450>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212656a",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e32b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing from config file\n",
    "conf = configparser.ConfigParser()\n",
    "conf.read(\"config\")\n",
    "data_source_path = conf.get(\"DATASOURCE PATH\", \"PATH\")\n",
    "\n",
    "# Schema\n",
    "schema = StructType([\n",
    "    StructField('vendor_name', StringType(), True), \n",
    "    StructField('Trip_Pickup_DateTime', StringType(), True), \n",
    "    StructField('Trip_Dropoff_DateTime', StringType(), True), \n",
    "    StructField('Passenger_Count', LongType(), True), \n",
    "    StructField('Trip_Distance', DoubleType(), True), \n",
    "    StructField('Rate_Code', DoubleType(), True), \n",
    "    StructField('store_and_forward', DoubleType(), True), \n",
    "    StructField('Payment_Type', StringType(), True), \n",
    "    StructField('Fare_Amt', DoubleType(), True), \n",
    "    StructField('surcharge', DoubleType(), True), \n",
    "    StructField('mta_tax', DoubleType(), True), \n",
    "    StructField('Tip_Amt', DoubleType(), True), \n",
    "    StructField('Tolls_Amt', DoubleType(), True), \n",
    "    StructField('Total_Amt', DoubleType(), True)]\n",
    ")\n",
    "\n",
    "# Reading the DataSource from PySpark\n",
    "df_full = spark.read.schema(schema).parquet(data_source_path, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abda0891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Partitons: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"No of Partitons: {df_full.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1306b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.01\n",
    "df = df_full.sample(fraction=frac, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae8bd3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_name: string (nullable = true)\n",
      " |-- Trip_Pickup_DateTime: string (nullable = true)\n",
      " |-- Trip_Dropoff_DateTime: string (nullable = true)\n",
      " |-- Passenger_Count: long (nullable = true)\n",
      " |-- Trip_Distance: double (nullable = true)\n",
      " |-- Rate_Code: double (nullable = true)\n",
      " |-- store_and_forward: double (nullable = true)\n",
      " |-- Payment_Type: string (nullable = true)\n",
      " |-- Fare_Amt: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- Tip_Amt: double (nullable = true)\n",
      " |-- Tolls_Amt: double (nullable = true)\n",
      " |-- Total_Amt: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21617cf",
   "metadata": {},
   "source": [
    "### Data Cleaning And Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c037c",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "964441b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Rows: 140868\n",
      "No of cols: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "no_of_row = df.count()\n",
    "print(f\"No of Rows: {no_of_row}\")\n",
    "print(f\"No of cols: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52345774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "null_count = df.\\\n",
    "select(\n",
    "    [F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    ").collect()[0]\\\n",
    ".asDict()\n",
    "null_col_list = [c for c in null_count if null_count[c] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84284dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vendor_name': 0,\n",
       " 'Trip_Pickup_DateTime': 0,\n",
       " 'Trip_Dropoff_DateTime': 0,\n",
       " 'Passenger_Count': 0,\n",
       " 'Trip_Distance': 0,\n",
       " 'Rate_Code': 140868,\n",
       " 'store_and_forward': 140862,\n",
       " 'Payment_Type': 0,\n",
       " 'Fare_Amt': 0,\n",
       " 'surcharge': 0,\n",
       " 'mta_tax': 140868,\n",
       " 'Tip_Amt': 0,\n",
       " 'Tolls_Amt': 0,\n",
       " 'Total_Amt': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d42230a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-------+\n",
      "|Rate_Code|store_and_forward|mta_tax|\n",
      "+---------+-----------------+-------+\n",
      "|   140868|           140862| 140868|\n",
      "+---------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We could see below three cols have huge no of Null Values\n",
    "# The Rate_Code and mta_tax is completely null\n",
    "# store_and_forward have few rows present\n",
    "null_col_list = [\"Rate_Code\", \"store_and_forward\", \"mta_tax\"]\n",
    "df.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in null_col_list]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "244ddec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-------+\n",
      "|Rate_Code|store_and_forward|mta_tax|\n",
      "+---------+-----------------+-------+\n",
      "|        0|                1|      0|\n",
      "+---------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking no of Distinct values in Null having columns\n",
    "df.select([F.countDistinct(F.col(c)).alias(c) for c in null_col_list]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa89a984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|store_and_forward| count|\n",
      "+-----------------+------+\n",
      "|              0.0|     6|\n",
      "|             null|140862|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count of distinct values on store_and_forward, we could see the values are very small compared to total rows\n",
    "df.select(F.col(\"store_and_forward\")).groupBy('store_and_forward').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e7a6d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the amount of null values is very large compared to total no of records we are dropping those columns\n",
    "df_not_null = df.drop(*null_col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d6705d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_name: string (nullable = true)\n",
      " |-- Trip_Pickup_DateTime: string (nullable = true)\n",
      " |-- Trip_Dropoff_DateTime: string (nullable = true)\n",
      " |-- Passenger_Count: long (nullable = true)\n",
      " |-- Trip_Distance: double (nullable = true)\n",
      " |-- Payment_Type: string (nullable = true)\n",
      " |-- Fare_Amt: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- Tip_Amt: double (nullable = true)\n",
      " |-- Tolls_Amt: double (nullable = true)\n",
      " |-- Total_Amt: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_not_null.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134eca3",
   "metadata": {},
   "source": [
    "#### Data Transformation and DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d44ad9f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|Trip_Pickup_DateTime|Trip_Dropoff_DateTime|\n",
      "+--------------------+---------------------+\n",
      "| 2009-01-03 11:05:27|  2009-01-03 11:10:55|\n",
      "| 2009-01-23 00:27:00|  2009-01-23 00:35:00|\n",
      "| 2009-01-18 16:49:45|  2009-01-18 16:56:34|\n",
      "+--------------------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Currently the DateTime col are in String format, will check the format\n",
    "date_col = [\"Trip_Pickup_DateTime\", \"Trip_Dropoff_DateTime\"]\n",
    "df_not_null.select(date_col).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42f40836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could see the format is %Y-%M-%D %H:%M:%s\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "date_dict_map = {date_c: F.to_timestamp(F.col(date_c)) for date_c in date_col}\n",
    "df_time_parsed = df_not_null.withColumns(date_dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46fbce92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_name: string (nullable = true)\n",
      " |-- Trip_Pickup_DateTime: timestamp (nullable = true)\n",
      " |-- Trip_Dropoff_DateTime: timestamp (nullable = true)\n",
      " |-- Passenger_Count: long (nullable = true)\n",
      " |-- Trip_Distance: double (nullable = true)\n",
      " |-- Payment_Type: string (nullable = true)\n",
      " |-- Fare_Amt: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- Tip_Amt: double (nullable = true)\n",
      " |-- Tolls_Amt: double (nullable = true)\n",
      " |-- Total_Amt: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time_parsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7843bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Duration Columns\n",
    "df_date_parsed = df_time_parsed.withColumn(\n",
    "    \"duration\", \n",
    "    F.col(\"Trip_Dropoff_DateTime\").cast(\"long\") - F.col(\"Trip_Pickup_DateTime\").cast(\"long\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8713bef",
   "metadata": {},
   "source": [
    "#### Distinct Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6e00422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joker021/.local/lib/python3.10/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "/home/joker021/.local/lib/python3.10/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_list` loads all data into the driver's memory. It should only be used if the resulting list is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "string_cols = [f.name for f in df_date_parsed.schema.fields if isinstance(f.dataType, F.StringType)]\n",
    "distinct_count = df_date_parsed.select([F.countDistinct(F.col(c)).alias(c) for c in string_cols])\n",
    "distinct_count_pd = distinct_count.pandas_api().transpose()\n",
    "dist_cols = distinct_count_pd[distinct_count_pd[0]<50].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08211500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|vendor_name|Payment_Type|\n",
      "+-----------+------------+\n",
      "|          3|           6|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date_parsed.select([F.countDistinct(F.col(c)).alias(c) for c in dist_cols]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fb5983c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|vendor_name|\n",
      "+-----------+\n",
      "|        CMT|\n",
      "|        VTS|\n",
      "|        DDS|\n",
      "+-----------+\n",
      "\n",
      "+------------+\n",
      "|Payment_Type|\n",
      "+------------+\n",
      "|   No Charge|\n",
      "|        CASH|\n",
      "|      Credit|\n",
      "|        Cash|\n",
      "|     Dispute|\n",
      "|      CREDIT|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View Distinct Values\n",
    "for c in dist_cols:\n",
    "    df_date_parsed.select(c).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f498fff",
   "metadata": {},
   "source": [
    "#### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecc7846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried distinct method and directly dropDup but both consumed lot of memory, \n",
    "# hence applying a tranformation and removing the duplicates which resulted in same result as direct dropDuplicates\n",
    "# but less mem consumption\n",
    "\n",
    "# We are concating all cols, and then checking dup based on concated col\n",
    "df_drop_by_concat = df_date_parsed\\\n",
    ".withColumn(\"concat_cols\", F.concat_ws(\"||\", *df_date_parsed.columns))\\\n",
    ".dropDuplicates([\"concat_cols\"])\\\n",
    ".drop(\"concat_cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00e55f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking count after duplicates\n",
    "cnt_after_drop = df_drop_by_concat.count()\n",
    "\n",
    "# No of Duplciates dropped\n",
    "no_of_row - cnt_after_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "180d0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_drop_by_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab8efe",
   "metadata": {},
   "source": [
    "### Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b1a0c",
   "metadata": {},
   "source": [
    "#### Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32b04c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Schema for Validation\n",
    "validate_schema = StructType(\n",
    "    [\n",
    "        StructField('vendor_name', StringType(), True), \n",
    "        StructField('Trip_Pickup_DateTime', TimestampType(), True), \n",
    "        StructField('Trip_Dropoff_DateTime', TimestampType(), True), \n",
    "        StructField('Passenger_Count', LongType(), True), \n",
    "        StructField('Trip_Distance', DoubleType(), True), \n",
    "        StructField('Payment_Type', StringType(), True), \n",
    "        StructField('Fare_Amt', DoubleType(), True), \n",
    "        StructField('surcharge', DoubleType(), True), \n",
    "        StructField('Tip_Amt', DoubleType(), True), \n",
    "        StructField('Tolls_Amt', DoubleType(), True),\n",
    "        StructField('Total_Amt', DoubleType(), True),\n",
    "        StructField('duration', LongType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8945c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Schema\n",
    "assert validate_schema == df_cleaned.schema, \"schema is not valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb33c67",
   "metadata": {},
   "source": [
    "#### Null Value Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4393f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method cosumes more memory hence commenting the below, It better to do one by one\n",
    "# is_null_values = df_cleaned.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_cleaned.columns]).collect()[0].asDict()\n",
    "# [col for col in is_null_values if is_null_values[col] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfd8667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There No Null columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for c in df_cleaned.columns:\n",
    "#     cnt = df_cleaned.select(c).where(F.col(c).isNull()).count()\n",
    "#     Experimenting below\n",
    "    cnt = df_cleaned.where(F.col(c).isNull()).select(c).count()\n",
    "    if cnt > 0:\n",
    "        print(c, cnt)\n",
    "else:\n",
    "    print(\"There No Null columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dc0925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data Range Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50de24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_numeric_cols = [\n",
    "#     f.name \n",
    "#     for f in df_cleaned.schema.fields \n",
    "#     if isinstance(f.dataType, DoubleType) or isinstance(f.dataType, LongType)\n",
    "# ]\n",
    "# bounds = {\n",
    "#     c: dict(\n",
    "#         zip([\"q1\", \"q3\"], df_cleaned.approxQuantile(c, [0.25, 0.75], 0.1))\n",
    "#     )\n",
    "#     for c in df_numeric_cols\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e2806c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in bounds:\n",
    "#     iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "#     bounds[c]['lower'] = bounds[c]['q1'] - (iqr * 1.5)\n",
    "#     bounds[c]['upper'] = bounds[c]['q3'] + (iqr * 1.5)\n",
    "# print(bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f252e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_outlier = df_cleaned.select(\n",
    "#     \"*\",\n",
    "#     *[\n",
    "#         F.when(\n",
    "#             F.col(c).between(bounds[c]['lower'], bounds[c]['upper']),\n",
    "#             0\n",
    "#         ).otherwise(1).alias(c+\"_out\") \n",
    "#         for c in df_numeric_cols\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "feb88a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_outlier.select([F.sum(c).alias(c) for c in df_outlier.columns if c.endswith(\"out\")]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa714d",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15323341",
   "metadata": {},
   "source": [
    "#### Data Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1955f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_name: string (nullable = true)\n",
      " |-- Trip_Pickup_DateTime: timestamp (nullable = true)\n",
      " |-- Trip_Dropoff_DateTime: timestamp (nullable = true)\n",
      " |-- Passenger_Count: long (nullable = true)\n",
      " |-- Trip_Distance: double (nullable = true)\n",
      " |-- Payment_Type: string (nullable = true)\n",
      " |-- Fare_Amt: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- Tip_Amt: double (nullable = true)\n",
      " |-- Tolls_Amt: double (nullable = true)\n",
      " |-- Total_Amt: double (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c02f87cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_vendor = df_cleaned.select(\"vendor_name\",*numeric_col)\\\n",
    ".groupBy(F.col(\"vendor_name\"))\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2caec9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 117:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------------------+-----------------+-------------------+------------------+-------------------+------------------+-----------------+\n",
      "|vendor_name|Passenger_Count_avg| Trip_Distance_avg|     Fare_Amt_avg|      surcharge_avg|       Tip_Amt_avg|      Tolls_Amt_avg|     Total_Amt_avg|     duration_avg|\n",
      "+-----------+-------------------+------------------+-----------------+-------------------+------------------+-------------------+------------------+-----------------+\n",
      "|        CMT| 1.3143077092268778|2.5235094185480196|9.554908014013968|                0.0|0.4324191293145434|0.11141176100925422|  10.1014725613109|641.4716972239243|\n",
      "|        VTS|  2.106358196351353| 2.555544341394518|9.383387592271744|0.32444709753505235|0.5146023033356143|0.11781663584875528|10.340923372595803|744.4733048934962|\n",
      "|        DDS| 1.3511137162954279| 2.720890973036341|9.770267291910915|  0.338862837045721|0.4148722157092615|0.13438452520515828| 10.65838686987105|769.7600234466588|\n",
      "+-----------+-------------------+------------------+-----------------+-------------------+------------------+-------------------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_vendor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41abbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_hour = df_cleaned.select(\"Trip_Pickup_DateTime\", *numeric_col)\\\n",
    ".groupBy(F.hour(F.col(\"Trip_Pickup_DateTime\")).alias(\"Hour\"))\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98a28ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 123:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------+------------------+-------------------+-------------------+--------------------+------------------+-----------------+\n",
      "|Hour|Passenger_Count_avg| Trip_Distance_avg|      Fare_Amt_avg|      surcharge_avg|        Tip_Amt_avg|       Tolls_Amt_avg|     Total_Amt_avg|     duration_avg|\n",
      "+----+-------------------+------------------+------------------+-------------------+-------------------+--------------------+------------------+-----------------+\n",
      "|  12| 1.6621863799283154|2.3294190561529287|  8.98279569892473|                0.0|0.42923685782556753|  0.1350089605734768|  9.55092443249702|708.0710872162485|\n",
      "|  22| 1.7998421883219358| 2.719929642293529|  9.77783403471858|0.27373750657548657| 0.5339281956864806| 0.08145055234087327|10.666950289321417|663.6657022619673|\n",
      "|   1| 1.8166666666666667| 3.021709595959597|10.258030303030312|0.26224747474747473| 0.5628308080808083|0.055479797979797976|11.138588383838394|686.8825757575758|\n",
      "+----+-------------------+------------------+------------------+-------------------+-------------------+--------------------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_hour.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f59399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_date = df_cleaned.select(\n",
    "    F.to_date(\"Trip_Pickup_DateTime\", \"yyyy-MM-dd\").alias(\"date\"), \n",
    "    *numeric_col\n",
    ")\\\n",
    ".groupBy(\"date\")\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcdaa437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 129:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|      date|Passenger_Count_avg|Trip_Distance_avg|     Fare_Amt_avg|      surcharge_avg|        Tip_Amt_avg|      Tolls_Amt_avg|     Total_Amt_avg|     duration_avg|\n",
      "+----------+-------------------+-----------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|2009-01-01| 1.8700092850510677|2.877926338594861|9.835369854534202|0.06499535747446611|0.30442587434230894|0.13930671618693907|10.353692355307954|685.7570411637264|\n",
      "|2009-01-30| 1.6989327841228234|2.572048305560754| 9.75008238157648|0.21924733196030705| 0.5304474817449917| 0.1259127504212695|10.630557947949814|754.1759970043063|\n",
      "|2009-01-22| 1.6182838813151563|2.543257818765037|9.633221732157178| 0.2118083400160385| 0.5555573376102646|0.12126303127506022|10.531273055332802| 678.713111467522|\n",
      "+----------+-------------------+-----------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_date.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ea4c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_week = df_cleaned.select(\"Trip_Pickup_DateTime\", *numeric_col)\\\n",
    ".groupBy(F.weekofyear(F.col(\"Trip_Pickup_DateTime\")).alias(\"WeekOfYear\"))\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "854c2f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 135:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|WeekOfYear|Passenger_Count_avg|Trip_Distance_avg|     Fare_Amt_avg|      surcharge_avg|        Tip_Amt_avg|      Tolls_Amt_avg|     Total_Amt_avg|     duration_avg|\n",
      "+----------+-------------------+-----------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|         1| 1.8763191503663372| 2.81293331988976|9.829425959534838|0.13201586341332258|0.38161053975935993|0.12571486186731176|10.472665860052409|698.9305639577872|\n",
      "|         3| 1.6801956876245696|2.441457752008214|9.384307241650024|0.18131303980189648|0.47661412091562444|0.11642266111010416|10.158657063477659|696.8836141813131|\n",
      "|         5|  1.663582871330907|2.486220196591955| 9.41574232168973|0.19807149416686506| 0.5031668990850651|0.10787150096935448|10.227199074861403|705.2540729907146|\n",
      "+----------+-------------------+-----------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_week.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4727a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_dayofmonth = df_cleaned.select(\"Trip_Pickup_DateTime\", *numeric_col)\\\n",
    ".groupBy(F.dayofmonth(F.col(\"Trip_Pickup_DateTime\")).alias(\"DayOfMonth\"))\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f630d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 141:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------------+----------------+-------------------+-------------------+-------------------+-----------------+-----------------+\n",
      "|DayOfMonth|Passenger_Count_avg| Trip_Distance_avg|    Fare_Amt_avg|      surcharge_avg|        Tip_Amt_avg|      Tolls_Amt_avg|    Total_Amt_avg|     duration_avg|\n",
      "+----------+-------------------+------------------+----------------+-------------------+-------------------+-------------------+-----------------+-----------------+\n",
      "|        31| 1.8038999264164828|2.4890231788079467|9.26937086092716|0.11966519499632082| 0.4307763061074319|0.06725533480500369|9.887067696835917|657.0198675496689|\n",
      "|        28| 1.6293159609120522|2.3341628664495118|9.11593919652552|0.21758957654723127| 0.5111248642779588|0.08086427795874053|9.934855591748102|722.7463626492943|\n",
      "|        27| 1.6310228233305157|2.4156635672020306|9.16749788672866| 0.2092138630600169|0.49590659340659343| 0.1177726120033813|9.990390955198654|684.5978444632291|\n",
      "+----------+-------------------+------------------+----------------+-------------------+-------------------+-------------------+-----------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_dayofmonth.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4548d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_dayofweek = df_cleaned.select(\"Trip_Pickup_DateTime\", *numeric_col)\\\n",
    ".groupBy(F.dayofweek(F.col(\"Trip_Pickup_DateTime\")).alias(\"dayofweek\"))\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b96d7965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 147:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|dayofweek|Passenger_Count_avg| Trip_Distance_avg|     Fare_Amt_avg|      surcharge_avg|        Tip_Amt_avg|      Tolls_Amt_avg|     Total_Amt_avg|     duration_avg|\n",
      "+---------+-------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|        1| 1.8005262191764058|2.8216930184176694|9.700611270880483|0.11185216912439576| 0.4656758245120234|0.12108242060821126| 10.40087376858593|678.7389708131922|\n",
      "|        6| 1.7117979797979799| 2.520908686868691| 9.51842747474747|0.22151515151515153|0.47601575757575765|0.11997777777777746|10.336986666666675| 710.664808080808|\n",
      "|        3| 1.6393324061196106|2.5091883171070943| 9.38501752433936|0.20845618915159944|0.48296856745479827|0.12769346314325428|10.204135744089022|692.8576912378303|\n",
      "+---------+-------------------+------------------+-----------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_dayofweek.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97fea850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt_per_payment_type = df_cleaned.select(\"Payment_Type\")\\\n",
    ".groupBy(F.col(\"Payment_Type\"))\\\n",
    ".count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b06489e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 153:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|Payment_Type|count|\n",
      "+------------+-----+\n",
      "|   No Charge|  374|\n",
      "|        CASH|60285|\n",
      "|      Credit|28750|\n",
      "|        Cash|49810|\n",
      "|     Dispute|   68|\n",
      "|      CREDIT| 1577|\n",
      "+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_cnt_per_payment_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "955301a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = [\"Passenger_Count\", \"Trip_Distance\", \"Fare_Amt\", \"surcharge\", \"Tip_Amt\", \"Tolls_Amt\", \"Total_Amt\", \"duration\"]\n",
    "df_per_payment_type = df_cleaned.select(\"Payment_Type\",*numeric_col)\\\n",
    ".groupBy(F.col(\"Payment_Type\"))\\\n",
    ".agg(*[F.mean(F.col(c)).alias(c+\"_avg\") for c in numeric_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9eee393e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 159:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+------------------+------------------+-------------------+--------------------+-------------------+------------------+-----------------+\n",
      "|Payment_Type|Passenger_Count_avg| Trip_Distance_avg|      Fare_Amt_avg|      surcharge_avg|         Tip_Amt_avg|      Tolls_Amt_avg|     Total_Amt_avg|     duration_avg|\n",
      "+------------+-------------------+------------------+------------------+-------------------+--------------------+-------------------+------------------+-----------------+\n",
      "|   No Charge| 1.2379679144385027|2.2401069518716574|10.134224598930484|                0.0| 0.03617647058823529|0.06914438502673798|10.239545454545457|698.6737967914438|\n",
      "|        CASH| 2.0316330762212824|  2.38219228663847| 8.872720079621816| 0.3229161482955959|                 0.0|0.08728705316413718| 9.283686323297683|661.1249232810816|\n",
      "|      Credit| 1.6828521739130435| 3.219395826086955|11.400533217391295|0.17965217391304347|   2.183159304347825|0.22185286956521838|13.988327999999978|922.1557913043479|\n",
      "|        Cash|  1.334430837181289|2.3424424412768516| 9.027776550893398|                0.0|0.001666332061834...|0.08419674764103573| 9.115326038947996|609.9599478016463|\n",
      "|     Dispute| 1.3235294117647058|2.9411764705882355|12.495588235294116|                0.0| 0.08794117647058824|              0.325|12.908529411764706|649.8088235294117|\n",
      "|      CREDIT| 1.2511097019657578| 3.470259987317692|12.066157260621434|0.34400760938490804|   2.244045656309449|0.28383005707038694|14.938040583386181|934.3925174381737|\n",
      "+------------+-------------------+------------------+------------------+-------------------+--------------------+-------------------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_per_payment_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d553cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt_payment_cnt_per_vendor = df_cleaned\\\n",
    ".select(\"vendor_name\", \"Payment_Type\")\\\n",
    ".groupBy(\"vendor_name\", \"Payment_Type\")\\\n",
    ".count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc45bbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 165:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----+\n",
      "|vendor_name|Payment_Type|count|\n",
      "+-----------+------------+-----+\n",
      "|        VTS|        CASH|53332|\n",
      "|        DDS|      CREDIT| 1577|\n",
      "|        CMT|      Credit|13399|\n",
      "|        DDS|        CASH| 6953|\n",
      "|        CMT|   No Charge|  374|\n",
      "|        CMT|        Cash|49810|\n",
      "|        VTS|      Credit|15351|\n",
      "|        CMT|     Dispute|   68|\n",
      "+-----------+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_cnt_payment_cnt_per_vendor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbaca11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
